# Pricing Configuration Guide

## Overview

The Steer LLM SDK provides automatic cost calculation for all API calls based on token usage and model-specific pricing. All pricing is centrally configured in the SDK to ensure accurate billing and rate limiting.

## Pricing Structure

Each model has three pricing components:

1. **Input tokens** - Cost per 1,000 tokens sent to the model
2. **Output tokens** - Cost per 1,000 tokens generated by the model  
3. **Cached input tokens** (optional) - Reduced cost for cached/repeated input tokens

## Current Pricing Source

All model pricing is configured in `steer_llm_sdk/config/models.py`. This is the single source of truth for pricing within the SDK.

Pricing was last verified: **2025-08-14**

Official pricing sources:
- OpenAI: https://platform.openai.com/pricing
- Anthropic: https://www.anthropic.com/pricing
- xAI: https://docs.x.ai/api/pricing

## Using Cost Calculation

The SDK automatically calculates costs for every API call:

```python
from steer_llm_sdk import SteerLLMClient

client = SteerLLMClient()

# Generate text
response = await client.generate(
    "Write a haiku about programming",
    "gpt-4o-mini"
)

# Access cost information
print(f"Total cost: ${response.cost_usd:.6f}")
print(f"Cost breakdown: {response.cost_breakdown}")
# Output:
# Total cost: $0.000045
# Cost breakdown: {
#   'input_cost': 0.000015,
#   'output_cost': 0.00003,
#   'cache_savings': 0.0,
#   'total_cost': 0.000045
# }
```

## Cache Savings

For models that support prompt caching (e.g., OpenAI GPT models, Anthropic Claude):

```python
response = await client.generate(
    messages,
    "gpt-4o-mini",
    # Enable caching if supported by the model
)

# If the model used cached tokens, savings are calculated automatically
if response.usage.get('cache_info', {}).get('cached_tokens'):
    print(f"Cache savings: ${response.cost_breakdown['cache_savings']:.6f}")
```

## Example: Cost Comparison

```python
from steer_llm_sdk import get_available_models

models = get_available_models()

# Compare costs across models
prompt_tokens = 1000
completion_tokens = 500

for model_id, config in models.items():
    if config.input_cost_per_1k_tokens and config.output_cost_per_1k_tokens:
        input_cost = config.input_cost_per_1k_tokens
        output_cost = config.output_cost_per_1k_tokens
        total_cost = input_cost + (completion_tokens / 1000) * output_cost
        
        print(f"{model_id}:")
        print(f"  Input: ${input_cost:.6f}/1k tokens")
        print(f"  Output: ${output_cost:.6f}/1k tokens")
        print(f"  Example cost: ${total_cost:.6f}")
        
        if config.cached_input_cost_per_1k_tokens:
            cache_savings = (input_cost - config.cached_input_cost_per_1k_tokens)
            print(f"  Cache savings: ${cache_savings:.6f}/1k tokens")
```

## Model Pricing Reference

### OpenAI Models
- **GPT-4o Mini**: $0.15/1M input, $0.60/1M output
- **GPT-4.1 Nano**: $0.10/1M input, $0.40/1M output
- **GPT-5 Mini**: $0.25/1M input, $2.00/1M output
- **O4 Mini**: $1.10/1M input, $4.40/1M output

### Anthropic Models
- **Claude 3 Haiku**: $0.30/1M input, $1.50/1M output
- **Claude 3.5 Haiku**: $1.00/1M input, $5.00/1M output

### Cache Pricing
Models that support caching typically charge 10-50% of regular input prices for cached tokens.

## Internal Development Note

Pricing configuration is managed internally by the SDK maintainers. For SDK development and testing purposes only, pricing overrides can be enabled with proper authorization. This feature is not available for general use to ensure accurate billing and rate limiting in production environments like SteerQA.